{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória de Dados (EDA) - Produtos HP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introdução\n",
    "Este notebook apresenta a Análise Exploratória de Dados (EDA) realizada sobre a base de dados de produtos HP coletada de diferentes fontes de e-commerce. O objetivo é entender a distribuição dos dados, identificar padrões, anomalias e extrair insights que possam ser úteis para futuras etapas de modelagem e classificação de produtos como \"originais\" ou \"suspeitos/piratas\".\n",
    "### Escopo do Projeto\n",
    "*   **Segmento de Produtos:** Cartuchos e Toners HP.\n",
    "*   **Abrangência de Sites:** Mercado Livre e Americanas (dados mockados para Americanas).\n",
    "*   **Período de Coleta:** Dados simulados até a data atual.\n",
    "*   **Objetivo Principal:** Fornecer uma base de dados limpa, enriquecida e analisada para o desenvolvimento de um modelo de classificação de autenticidade de produtos HP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Função para carregar o dataset mais recente\n",
    "def get_latest_csv_file(pattern):\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        return None\n",
    "    return max(files, key=os.path.getctime)\n",
    "\n",
    "latest_structured_dataset = get_latest_csv_file(\"hp_products_structured_dataset_mock_*.csv\")\n",
    "latest_labeled_dataset = get_latest_csv_file(\"hp_products_labeled_mock_*.csv\")\n",
    "\n",
    "if latest_structured_dataset:\n",
    "    df = pd.read_csv(latest_structured_dataset)\n",
    "    print(f\"Dataset estruturado carregado: {latest_structured_dataset}\")\n",
    "else:\n",
    "    print(\"Nenhum dataset estruturado mockado encontrado.\")\n",
    "    df = pd.DataFrame() # Cria um DataFrame vazio para evitar erros\n",
    "\n",
    "if latest_labeled_dataset:\n",
    "    df_labeled = pd.read_csv(latest_labeled_dataset)\n",
    "    print(f\"Dataset rotulado carregado: {latest_labeled_dataset}\")\n",
    "else:\n",
    "    print(\"Nenhum dataset rotulado mockado encontrado.\")\n",
    "    df_labeled = pd.DataFrame() # Cria um DataFrame vazio para evitar erros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limpeza da Base\n",
    "A limpeza da base de dados é uma etapa crucial para garantir a qualidade e a consistência dos dados. As principais operações de limpeza realizadas incluem:\n",
    "*   **Tratamento de Valores Ausentes:** Identificação e tratamento de valores nulos ou ausentes em colunas críticas.\n",
    "*   **Remoção de Duplicatas:** Eliminação de registros duplicados para evitar viés nas análises.\n",
    "*   **Padronização de Formatos:** Conversão de tipos de dados e padronização de formatos (ex: preços para numérico, texto para minúsculas).\n",
    "*   **Correção de Erros:** Identificação e correção de erros de digitação ou inconsistências nos dados.\n",
    "\n",
    "_Nota: Para este entregável, a limpeza foi simulada com base nos dados mockados, assumindo que as etapas de limpeza e enriquecimento anteriores já foram realizadas, conforme o Entregável 2 original. O foco aqui é a análise exploratória._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de verificação de valores ausentes\n",
    "if not df.empty:\n",
    "    print(\"\nValores Ausentes:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "# Exemplo de verificação de duplicatas\n",
    "if not df.empty:\n",
    "    print(\"\nDuplicatas:\")\n",
    "    print(df.duplicated().sum())\n",
    "\n",
    "# Exemplo de conversão de tipo (já assumido como numérico para price_numeric)\n",
    "if \"price_numeric\" in df.columns:\n",
    "    print(f\"\nTipo de dado da coluna \"price_numeric\": {df[\"price_numeric\"].dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análise Exploratória de Dados (EDA)\n",
    "### 3.1. Distribuição de Preços\n",
    "A análise da distribuição de preços nos ajuda a entender a faixa de valores dos produtos e identificar possíveis outliers ou padrões de precificação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"price_numeric\" in df.columns and not df[\"price_numeric\"].isnull().all():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df[\"price_numeric\"].dropna(), kde=True, bins=30)\n",
    "    plt.title(\"Distribuição de Preços\")\n",
    "    plt.xlabel(\"Preço (R$)\")\n",
    "    plt.ylabel(\"Frequência\")\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Dados de preço não disponíveis para análise de distribuição.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Frequência de Palavras em Títulos e Descrições (Wordclouds e N-grams)\n",
    "A análise textual nos ajuda a identificar os termos mais comuns utilizados nos títulos e descrições dos produtos, revelando características importantes e possíveis palavras-chave para classificação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "all_titles = \" \".join(df_labeled[\"title\"].dropna().astype(str))\n",
    "all_descriptions = \" \".join(df_labeled[\"description\"].dropna().astype(str))\n",
    "\n",
    "# WordCloud para Títulos\n",
    "if all_titles:\n",
    "    wordcloud_titles = WordCloud(width=800, height=400, background_color=\"white\", stopwords=stop_words).generate(all_titles)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud_titles, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud - Títulos de Produtos\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Dados de título não disponíveis para geração de word cloud.\")\n",
    "\n",
    "# WordCloud para Descrições\n",
    "if all_descriptions:\n",
    "    wordcloud_descriptions = WordCloud(width=800, height=400, background_color=\"white\", stopwords=stop_words).generate(all_descriptions)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud_descriptions, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud - Descrições de Produtos\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Dados de descrição não disponíveis para geração de word cloud.\")\n",
    "\n",
    "# N-grams para Títulos\n",
    "def get_ngrams(text, n):\n",
    "    tokens = word_tokenize(text.lower(), language='portuguese')\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "all_title_ngrams = []\n",
    "if not df_labeled.empty and \"title\" in df_labeled.columns:\n",
    "    for title in df_labeled[\"title\"].dropna().astype(str):\n",
    "        all_title_ngrams.extend(get_ngrams(title, 2)) # Bigrams\n",
    "\n",
    "if all_title_ngrams:\n",
    "    bigram_counts = Counter(all_title_ngrams)\n",
    "    most_common_bigrams = bigram_counts.most_common(10)\n",
    "    print(\"\nBigramas mais comuns em Títulos:\")\n",
    "    for bigram, count in most_common_bigrams:\n",
    "        print(f\"  {bigram}: {count}\")\n",
    "else:\n",
    "    print(\"Nenhum bigrama encontrado para títulos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Correlações entre Variáveis Numéricas\n",
    "A matriz de correlação nos ajuda a entender a relação entre as variáveis numéricas do dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\"price_numeric\", \"rating_numeric\", \"reviews_count\"]\n",
    "df_numeric = df[numeric_cols].copy()\n",
    "df_numeric = df_numeric.dropna()\n",
    "\n",
    "if not df_numeric.empty and len(df_numeric.columns) > 1:\n",
    "    correlation_matrix = df_numeric.corr()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Matriz de Correlação entre Variáveis Numéricas\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Dados numéricos insuficientes para análise de correlação.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Distribuição dos Anúncios por Rótulo\n",
    "Esta análise mostra a proporção de produtos classificados como \"original\" e \"suspeito/pirata\" com base nos critérios heurísticos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"label\" in df_labeled.columns and not df_labeled[\"label\"].isnull().all():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(data=df_labeled, x=\"label\", palette=\"viridis\")\n",
    "    plt.title(\"Distribuição de Anúncios por Rótulo\")\n",
    "    plt.xlabel(\"Rótulo\")\n",
    "    plt.ylabel(\"Contagem\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Dados de rótulo não disponíveis para análise de distribuição.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Identificação de Features Úteis para Modelagem\n",
    "Com base na EDA, podemos identificar features que são potencialmente úteis para a construção de modelos de Machine Learning para classificar produtos como originais ou suspeitos.\n",
    "*   **Features Numéricas:** `price_numeric`, `rating_numeric`, `reviews_count`.\n",
    "*   **Features Categóricas:** `platform`, `product_type`, `seller_name`.\n",
    "*   **Features Booleanas (Heurísticas):** `is_original`, `is_xl`, `has_sealed_info`, `has_invoice_info`, `has_warranty_info`.\n",
    "*   **Features Textuais (para PLN):** `title`, `description` (podem ser usadas para extração de features como TF-IDF, embeddings, etc.).\n",
    "*   **Feature Target:** `target_is_original` (rótulo binário: 1 para original, 0 para suspeito/pirata).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cronograma Macro do Projeto (CRISP-DM)\n",
    "A metodologia CRISP-DM (Cross-Industry Standard Process for Data Mining) é um modelo amplamente utilizado para guiar projetos de mineração de dados. Abaixo, um cronograma macro adaptado para este projeto:\n",
    "\n",
    "### 4.1. Entendimento do Negócio (Business Understanding)\n",
    "*   **Objetivo:** Definir os objetivos do projeto do ponto de vista do negócio (classificar produtos HP como originais ou suspeitos/piratas).\n",
    "*   **Atividades:** Reuniões com stakeholders, levantamento de requisitos, definição de critérios de sucesso.\n",
    "*   **Prazo:** 1 semana.\n",
    "\n",
    "### 4.2. Entendimento dos Dados (Data Understanding)\n",
    "*   **Objetivo:** Coletar, explorar e verificar a qualidade dos dados.\n",
    "*   **Atividades:** Web scraping (Mercado Livre, Americanas), EDA (este notebook), verificação de consistência dos dados.\n",
    "*   **Prazo:** 2 semanas.\n",
    "\n",
    "### 4.3. Preparação dos Dados (Data Preparation)\n",
    "*   **Objetivo:** Limpar, transformar e enriquecer os dados para a modelagem.\n",
    "*   **Atividades:** Tratamento de valores ausentes, remoção de duplicatas, padronização, criação de features derivadas (ex: `price_numeric`, `is_original`), rotulagem heurística.\n",
    "*   **Prazo:** 3 semanas.\n",
    "\n",
    "### 4.4. Modelagem (Modeling)\n",
    "*   **Objetivo:** Selecionar e aplicar técnicas de modelagem para construir o modelo de classificação.\n",
    "*   **Atividades:** Seleção de algoritmos (ex: Random Forest, SVM, Redes Neurais), treinamento do modelo, validação cruzada, ajuste de hiperparâmetros.\n",
    "*   **Prazo:** 4 semanas.\n",
    "\n",
    "### 4.5. Avaliação (Evaluation)\n",
    "*   **Objetivo:** Avaliar a performance do modelo e sua capacidade de atender aos objetivos de negócio.\n",
    "*   **Atividades:** Análise de métricas (precisão, recall, F1-score, curva ROC), interpretação dos resultados, identificação de vieses.\n",
    "*   **Prazo:** 2 semanas.\n",
    "\n",
    "### 4.6. Implantação (Deployment)\n",
    "*   **Objetivo:** Integrar o modelo ao ambiente de produção e monitorar seu desempenho.\n",
    "*   **Atividades:** Desenvolvimento de API para o modelo, integração com sistemas existentes, monitoramento contínuo, manutenção.\n",
    "*   **Prazo:** 3 semanas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusão\n",
    "Este notebook demonstrou as etapas iniciais de EDA e preparação de dados, fornecendo insights valiosos e um dataset pronto para as próximas fases do projeto de classificação de produtos HP. O cronograma CRISP-DM serve como um guia para as atividades futuras, garantindo uma abordagem estruturada e eficiente.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}



